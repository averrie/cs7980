{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Preprocessing and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, gzip, io, random\n",
                "from pathlib import Path\n",
                "from typing import List, Optional\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import html\n",
                "from bs4 import BeautifulSoup\n",
                "\n",
                "RANDOM_SEED = 42\n",
                "rng = np.random.default_rng(RANDOM_SEED)\n",
                "random.seed(RANDOM_SEED)\n",
                "\n",
                "DATA_DIRS = [\n",
                "    r\".\\\\bluesky\\\\dataset\\\\labeled_all_posts.json\",\n",
                "    r\".\\\\truthsocial\\\\dataset\\\\labeled_all_posts.json\",\n",
                "]\n",
                "\n",
                "LABELS = [\"Left\",\"Right\",\"Neutral\"]\n",
                "LABEL2ID = {k:i for i,k in enumerate(LABELS)}\n",
                "ID2LABEL = {v:k for k,v in LABEL2ID.items()}\n",
                "\n",
                "def _open_text(path: Path):\n",
                "    if str(path).endswith(\".gz\"):\n",
                "        return io.TextIOWrapper(gzip.open(path, \"rb\"), encoding=\"utf-8\", errors=\"ignore\")\n",
                "    return open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\")\n",
                "\n",
                "def _iter_paths(root: Path):\n",
                "    ok = (\".json\",\".jsonl\",\".ndjson\",\".json.gz\",\".jsonl.gz\",\".ndjson.gz\")\n",
                "    for p in root.rglob(\"*\"):\n",
                "        if any(\"\".join(p.suffixes).lower().endswith(ext) for ext in ok):\n",
                "            yield p\n",
                "\n",
                "def _iter_records(path: Path):\n",
                "    with _open_text(path) as f:\n",
                "        head = f.read(2048)\n",
                "        f.seek(0)\n",
                "        first = head.lstrip()[:1]\n",
                "        if first == \"[\":\n",
                "            try:\n",
                "                data = json.load(f)\n",
                "                if isinstance(data, list):\n",
                "                    for x in data:\n",
                "                        if isinstance(x, dict): yield x\n",
                "                elif isinstance(data, dict):\n",
                "                    yield data\n",
                "            except Exception:\n",
                "                for line in f:\n",
                "                    line=line.strip()\n",
                "                    if not line: continue\n",
                "                    try:\n",
                "                        x=json.loads(line)\n",
                "                        if isinstance(x, dict): yield x\n",
                "                    except: pass\n",
                "        else:\n",
                "            for line in f:\n",
                "                line=line.strip()\n",
                "                if not line: continue\n",
                "                try:\n",
                "                    x=json.loads(line)\n",
                "                    if isinstance(x, dict): yield x\n",
                "                except: pass\n",
                "\n",
                "def _clean_text(html_content):\n",
                "    \"\"\"\n",
                "    Strips all HTML tags and unescapes entities from a string.\n",
                "    \"\"\"\n",
                "    if not html_content or not isinstance(html_content, str):\n",
                "        return None\n",
                "\n",
                "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
                "    text_with_entities = soup.get_text(separator=\" \", strip=True)\n",
                "    clean_text = html.unescape(text_with_entities)\n",
                "    return clean_text or None\n",
                "\n",
                "def _extract_text(rec: dict) -> Optional[str]:\n",
                "    \"\"\"\n",
                "    Heuristics across Bluesky/TruthSocial scrapes.\n",
                "    Try your common fields here; add more if needed.\n",
                "    \"\"\"\n",
                "    \n",
                "    # Bluesky record style:\n",
                "    if \"record\" in rec:\n",
                "        record = rec.get(\"record\") or {}\n",
                "        text = record.get(\"text\")\n",
                "        if isinstance(text, str) and text.strip():\n",
                "            return text.strip()\n",
                "        \n",
                "    # Truth social:\n",
                "    if \"content\" in rec:\n",
                "        cleaned = _clean_text(rec.get(\"content\"))\n",
                "        if cleaned:\n",
                "            return cleaned\n",
                "    \n",
                "    return None\n",
                "\n",
                "def load_dataframe(roots: List[str], min_len: int = 5, include_topic_prefix: bool = True) -> pd.DataFrame:\n",
                "    rows = []\n",
                "    for d in roots:\n",
                "        root = Path(d)\n",
                "        if not root.exists():\n",
                "            print(f\"[WARN] Missing: {root}\")\n",
                "            continue\n",
                "\n",
                "        if root.is_file():\n",
                "            paths = [root]\n",
                "        else:\n",
                "            paths = _iter_paths(root)\n",
                "\n",
                "        for path in paths:\n",
                "            for rec in _iter_records(path):\n",
                "                meta = rec.get(\"__meta__\", {}) or {}\n",
                "                label = str(meta.get(\"llm_label\") or \"\").strip().capitalize()\n",
                "                if label not in LABELS:  # skip unknown labels\n",
                "                    continue\n",
                "                txt = _extract_text(rec)\n",
                "                if not txt: \n",
                "                    continue\n",
                "                \n",
                "                topic = meta.get(\"topic\") or \"\"\n",
                "                if include_topic_prefix and topic:\n",
                "                    txt = f\"Topic: {topic}. Post: {txt}\"\n",
                "\n",
                "                platform = meta.get(\"platform\") or \"\"\n",
                "                matched_keyword = meta.get(\"matched_keyword\") or \"\"\n",
                "                \n",
                "                author_did = \"\"\n",
                "\n",
                "                if platform == \"bluesky\":\n",
                "                    author = rec.get(\"author\") or {}\n",
                "                    did = author.get(\"did\")\n",
                "                    if did:\n",
                "                        # prefix to avoid collisions across platforms\n",
                "                        author_did = f\"bsky:{did}\"\n",
                "                elif platform == \"truthsocial\":\n",
                "                    account = rec.get(\"account\") or {}\n",
                "                    acc_id = account.get(\"id\")\n",
                "                    if acc_id:\n",
                "                        author_did = f\"truth:{acc_id}\"\n",
                "                \n",
                "                post_id = rec.get(\"id\") or rec.get(\"cid\") or \"\"\n",
                "\n",
                "                if len(txt) < min_len:\n",
                "                    continue\n",
                "\n",
                "                rows.append({\n",
                "                    \"text\": txt,\n",
                "                    \"label\": label,\n",
                "                    \"y\": LABEL2ID[label],\n",
                "                    \"topic\": topic,\n",
                "                    \"platform\": platform,\n",
                "                    \"matched_keyword\": matched_keyword,\n",
                "                    \"author_did\": author_did,\n",
                "                    \"post_id\": post_id,\n",
                "                })\n",
                "    df = pd.DataFrame(rows).drop_duplicates(subset=[\"text\",\"post_id\"], keep=\"first\")\n",
                "    print(df.shape, df[\"label\"].value_counts(dropna=False))\n",
                "    return df\n",
                "\n",
                "def stratified_splits(df: pd.DataFrame, test_size=0.2, val_size=0.1, group_col: Optional[str]=None, seed=RANDOM_SEED):\n",
                "    \"\"\"\n",
                "    Returns df_train, df_val, df_test.\n",
                "    If group_col is provided (e.g., 'author_did' or 'platform'), we avoid leakage by grouping.\n",
                "    \"\"\"\n",
                "    from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
                "\n",
                "    if group_col and group_col in df.columns and df[group_col].astype(bool).any():\n",
                "        gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
                "        idx = np.arange(len(df))\n",
                "        train_idx, test_idx = next(gss.split(idx, groups=df[group_col]))\n",
                "        df_train_full, df_test = df.iloc[train_idx], df.iloc[test_idx]\n",
                "\n",
                "        # val from train_full\n",
                "        gss2 = GroupShuffleSplit(n_splits=1, test_size=val_size/(1.0-test_size), random_state=seed)\n",
                "        idx2 = np.arange(len(df_train_full))\n",
                "        tr_idx, val_idx = next(gss2.split(idx2, groups=df_train_full[group_col].values))\n",
                "        df_train, df_val = df_train_full.iloc[tr_idx], df_train_full.iloc[val_idx]\n",
                "    else:\n",
                "        from sklearn.model_selection import train_test_split\n",
                "        df_train_full, df_test = train_test_split(\n",
                "            df, test_size=test_size, random_state=seed, stratify=df[\"y\"]\n",
                "        )\n",
                "        df_train, df_val = train_test_split(\n",
                "            df_train_full, test_size=val_size/(1.0-test_size), random_state=seed, stratify=df_train_full[\"y\"]\n",
                "        )\n",
                "    for name, part in [(\"train\",df_train),(\"val\",df_val),(\"test\",df_test)]:\n",
                "        print(name, part.shape, part[\"label\"].value_counts())\n",
                "    return df_train.reset_index(drop=True), df_val.reset_index(drop=True), df_test.reset_index(drop=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(131808, 8) label\n",
                        "Neutral    53778\n",
                        "Left       44332\n",
                        "Right      33698\n",
                        "Name: count, dtype: int64\n",
                        "train (93014, 8) label\n",
                        "Neutral    38107\n",
                        "Left       30928\n",
                        "Right      23979\n",
                        "Name: count, dtype: int64\n",
                        "val (13240, 8) label\n",
                        "Neutral    5448\n",
                        "Left       4407\n",
                        "Right      3385\n",
                        "Name: count, dtype: int64\n",
                        "test (25554, 8) label\n",
                        "Neutral    10223\n",
                        "Left        8997\n",
                        "Right       6334\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# load and split dataset\n",
                "df = load_dataframe(DATA_DIRS, include_topic_prefix=True)\n",
                "df_train, df_val, df_test = stratified_splits(df, group_col=\"author_did\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Naive Bayes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(131808, 8) label\n",
                        "Neutral    53778\n",
                        "Left       44332\n",
                        "Right      33698\n",
                        "Name: count, dtype: int64\n",
                        "train (93014, 8) label\n",
                        "Neutral    38107\n",
                        "Left       30928\n",
                        "Right      23979\n",
                        "Name: count, dtype: int64\n",
                        "val (13240, 8) label\n",
                        "Neutral    5448\n",
                        "Left       4407\n",
                        "Right      3385\n",
                        "Name: count, dtype: int64\n",
                        "test (25554, 8) label\n",
                        "Neutral    10223\n",
                        "Left        8997\n",
                        "Right       6334\n",
                        "Name: count, dtype: int64\n",
                        "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
                        "Best params: {'nb__alpha': 0.1}\n",
                        "Best Mean Cross-Validation Score on train: 0.3756423319278246\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "        Left      0.592     0.765     0.667      8997\n",
                        "     Neutral      0.715     0.583     0.643     10223\n",
                        "       Right      0.671     0.593     0.630      6334\n",
                        "\n",
                        "    accuracy                          0.650     25554\n",
                        "   macro avg      0.660     0.647     0.646     25554\n",
                        "weighted avg      0.661     0.650     0.648     25554\n",
                        "\n",
                        "[[6881  675 1441]\n",
                        " [1648 3755  931]\n",
                        " [3097 1164 5962]]\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.naive_bayes import ComplementNB\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "import joblib\n",
                "\n",
                "# pipeline\n",
                "pipe = Pipeline([\n",
                "    (\"tfidf\", TfidfVectorizer(\n",
                "        lowercase=True,\n",
                "        ngram_range=(1,2),\n",
                "        min_df=5,\n",
                "        max_df=0.7,\n",
                "        strip_accents=\"unicode\",\n",
                "        sublinear_tf=True,\n",
                "        token_pattern=r\"[A-Za-z][A-Za-z0-9_\\-']+\"\n",
                "    )),\n",
                "    (\"nb\", ComplementNB(alpha=0.5))\n",
                "])\n",
                "\n",
                "param_grid = {\"nb__alpha\": [0.1, 0.3, 0.5, 1.0]}\n",
                "gs = GridSearchCV(pipe, param_grid, scoring=\"f1_macro\", cv=3, n_jobs=-1, verbose=1)\n",
                "gs.fit(df_train[\"text\"], df_train[\"label\"])\n",
                "\n",
                "print(\"Best params:\", gs.best_params_)\n",
                "print(\"Best Mean Cross-Validation Score on train:\", gs.best_score_)\n",
                "\n",
                "# evaluate on test\n",
                "best = gs.best_estimator_\n",
                "y_pred = best.predict(df_test[\"text\"])\n",
                "print(classification_report(df_test[\"label\"], y_pred, digits=3))\n",
                "print(confusion_matrix(df_test[\"label\"], y_pred, labels=LABELS))\n",
                "\n",
                "# saving artifacts\n",
                "os.makedirs(\"models_nb\", exist_ok=True)\n",
                "joblib.dump(best, \"models_nb/nb_tfidf.joblib\")\n",
                "with open(\"models_nb/labels.txt\",\"w\") as f: f.write(\"\\n\".join(LABELS))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### BERT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Averrie\\OneDrive\\Programming\\northeastern\\cs7980\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "Map: 100%|██████████| 93014/93014 [00:05<00:00, 16972.69 examples/s]\n",
                        "Map: 100%|██████████| 13240/13240 [00:00<00:00, 21028.37 examples/s]\n",
                        "Map: 100%|██████████| 25554/25554 [00:01<00:00, 17054.36 examples/s]\n",
                        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                        "C:\\Users\\Averrie\\AppData\\Local\\Temp\\ipykernel_26520\\3641018292.py:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
                        "  super().__init__(*args, **kwargs)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='8721' max='8721' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [8721/8721 11:16, Epoch 3/3]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>200</td>\n",
                            "      <td>0.956600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>400</td>\n",
                            "      <td>0.830600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>600</td>\n",
                            "      <td>0.767700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>800</td>\n",
                            "      <td>0.755800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>1000</td>\n",
                            "      <td>0.714700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>1200</td>\n",
                            "      <td>0.703000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>1400</td>\n",
                            "      <td>0.697100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>1600</td>\n",
                            "      <td>0.645100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>1800</td>\n",
                            "      <td>0.646500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2000</td>\n",
                            "      <td>0.642600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2200</td>\n",
                            "      <td>0.629500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2400</td>\n",
                            "      <td>0.649900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2600</td>\n",
                            "      <td>0.644500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2800</td>\n",
                            "      <td>0.634500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3000</td>\n",
                            "      <td>0.585800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3200</td>\n",
                            "      <td>0.532000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3400</td>\n",
                            "      <td>0.537700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3600</td>\n",
                            "      <td>0.519500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3800</td>\n",
                            "      <td>0.520800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4000</td>\n",
                            "      <td>0.533600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4200</td>\n",
                            "      <td>0.516900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4400</td>\n",
                            "      <td>0.527400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4600</td>\n",
                            "      <td>0.506100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4800</td>\n",
                            "      <td>0.487800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5000</td>\n",
                            "      <td>0.504000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5200</td>\n",
                            "      <td>0.520300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5400</td>\n",
                            "      <td>0.492400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5600</td>\n",
                            "      <td>0.498700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5800</td>\n",
                            "      <td>0.480700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6000</td>\n",
                            "      <td>0.425700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6200</td>\n",
                            "      <td>0.400400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6400</td>\n",
                            "      <td>0.399200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6600</td>\n",
                            "      <td>0.402500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6800</td>\n",
                            "      <td>0.399400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7000</td>\n",
                            "      <td>0.407600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7200</td>\n",
                            "      <td>0.399600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7400</td>\n",
                            "      <td>0.390900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7600</td>\n",
                            "      <td>0.402800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7800</td>\n",
                            "      <td>0.404900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8000</td>\n",
                            "      <td>0.413200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8200</td>\n",
                            "      <td>0.396700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8400</td>\n",
                            "      <td>0.383700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8600</td>\n",
                            "      <td>0.396800</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='607' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [207/207 00:35]\n",
                            "    </div>\n",
                            "    "
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Val metrics: {'eval_loss': 0.6404696106910706, 'eval_accuracy': 0.7536253776435046, 'eval_f1': 0.751164508545172, 'eval_f1_Left': 0.74834728955487, 'eval_f1_Right': 0.7342911043389929, 'eval_f1_Neutral': 0.7708551317416532, 'eval_runtime': 25.664, 'eval_samples_per_second': 515.898, 'eval_steps_per_second': 8.066, 'epoch': 3.0}\n",
                        "Test metrics: {'eval_loss': 0.6434869170188904, 'eval_accuracy': 0.7495108397902481, 'eval_f1': 0.7475576787316847, 'eval_f1_Left': 0.7557268483335142, 'eval_f1_Right': 0.731244211176289, 'eval_f1_Neutral': 0.7557019766852509, 'eval_runtime': 29.6035, 'eval_samples_per_second': 863.21, 'eval_steps_per_second': 13.512, 'epoch': 3.0}\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "('models_distilbert/best\\\\tokenizer_config.json',\n",
                            " 'models_distilbert/best\\\\special_tokens_map.json',\n",
                            " 'models_distilbert/best\\\\vocab.txt',\n",
                            " 'models_distilbert/best\\\\added_tokens.json',\n",
                            " 'models_distilbert/best\\\\tokenizer.json')"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import torch, numpy as np\n",
                "from datasets import Dataset, DatasetDict\n",
                "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
                "                        DataCollatorWithPadding, TrainingArguments, Trainer)\n",
                "from sklearn.metrics import accuracy_score, f1_score\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "MODEL_NAME = \"distilbert-base-uncased\"\n",
                "MAX_LEN = 256\n",
                "\n",
                "# build HF datasets\n",
                "def to_hf(df):\n",
                "    # keep numeric label and rename it to 'labels'\n",
                "    tmp = df[[\"text\", \"y\", \"topic\", \"platform\"]].copy()\n",
                "    tmp = tmp.rename(columns={\"y\": \"labels\"})\n",
                "    return Dataset.from_pandas(tmp, preserve_index=False)\n",
                "\n",
                "hf = DatasetDict({\n",
                "    \"train\": to_hf(df_train),\n",
                "    \"validation\": to_hf(df_val),\n",
                "    \"test\": to_hf(df_test),\n",
                "})\n",
                "\n",
                "\n",
                "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
                "\n",
                "def tokenize(batch):\n",
                "    return tok(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
                "\n",
                "hf_tok = hf.map(\n",
                "    tokenize,\n",
                "    batched=True,\n",
                "    remove_columns=[\"text\", \"topic\", \"platform\"],  # 'labels' is kept\n",
                ")\n",
                "data_collator = DataCollatorWithPadding(tokenizer=tok)\n",
                "\n",
                "def compute_metrics(pred):\n",
                "    logits, labels = pred\n",
                "    preds = np.argmax(logits, axis=-1)\n",
                "\n",
                "    acc = accuracy_score(labels, preds)\n",
                "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
                "\n",
                "    out = {\"accuracy\": acc, \"f1\": f1_macro}\n",
                "\n",
                "    # per-class F1\n",
                "    for i, name in ID2LABEL.items():\n",
                "        out[f\"f1_{name}\"] = f1_score(\n",
                "            (labels == i).astype(int),\n",
                "            (preds == i).astype(int),\n",
                "            average=\"binary\",\n",
                "            zero_division=0,\n",
                "        )\n",
                "    return out\n",
                "\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    num_labels=len(LABELS),\n",
                "    id2label=ID2LABEL,\n",
                "    label2id=LABEL2ID\n",
                ").to(device)\n",
                "\n",
                "args = TrainingArguments(\n",
                "    output_dir=\"models_distilbert\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=32,\n",
                "    per_device_eval_batch_size=64,\n",
                "    num_train_epochs=3,\n",
                "    weight_decay=0.01,\n",
                "    logging_steps=200,\n",
                "    save_steps=2000,\n",
                "    seed=RANDOM_SEED,\n",
                "    dataloader_num_workers=4,\n",
                ")\n",
                "\n",
                "# handle imbalance with class weights\n",
                "use_class_weights = True\n",
                "class_counts = df_train[\"y\"].value_counts().reindex(range(len(LABELS)), fill_value=0).values\n",
                "weights = torch.tensor(len(df_train)/np.maximum(class_counts,1), dtype=torch.float32, device=device)\n",
                "weights = weights / weights.sum() * len(LABELS)  # normalize around 1\n",
                "\n",
                "class WeightedTrainer(Trainer):\n",
                "    def __init__(self, *args, class_weights=None, **kwargs):\n",
                "        super().__init__(*args, **kwargs)\n",
                "        self.class_weights = class_weights\n",
                "\n",
                "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
                "        # labels\n",
                "        labels = inputs.get(\"labels\")\n",
                "        outputs = model(**inputs)\n",
                "        logits = outputs.get(\"logits\")\n",
                "\n",
                "        if self.class_weights is not None:\n",
                "            loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
                "        else:\n",
                "            loss_fct = torch.nn.CrossEntropyLoss()\n",
                "\n",
                "        loss = loss_fct(\n",
                "            logits.view(-1, model.config.num_labels),\n",
                "            labels.view(-1)\n",
                "        )\n",
                "        return (loss, outputs) if return_outputs else loss\n",
                "\n",
                "trainer = WeightedTrainer(\n",
                "    model=model,\n",
                "    args=args,\n",
                "    train_dataset=hf_tok[\"train\"],\n",
                "    eval_dataset=hf_tok[\"validation\"],\n",
                "    tokenizer=tok,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics,\n",
                "    class_weights=weights,\n",
                ")\n",
                "\n",
                "trainer.train()\n",
                "print(\"Val metrics:\", trainer.evaluate(hf_tok[\"validation\"]))\n",
                "print(\"Test metrics:\", trainer.evaluate(hf_tok[\"test\"]))\n",
                "\n",
                "# saving model\n",
                "trainer.save_model(\"models_distilbert/best\")\n",
                "tok.save_pretrained(\"models_distilbert/best\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}