{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c019a2",
   "metadata": {},
   "source": [
    "# Political Bias EDA & Tokenization Pipeline\n",
    "\n",
    "This notebook performs end-to-end exploratory data analysis (EDA) and tokenization for your TruthSocial/Bluesky datasets.\n",
    "\n",
    "**What it does:**\n",
    "1. Discover and load merged datasets (global and per topic)\n",
    "2. Clean HTML → plain text\n",
    "3. Tokenize with `nltk.tokenize`\n",
    "4. Global EDA: length distribution, top words, platform/topic counts\n",
    "5. Sentiment (VADER)\n",
    "6. Semantic embeddings + UMAP visualization\n",
    "7. TF–IDF keyword comparison by platform\n",
    "8. Topic-level EDA (loop over each topic file)\n",
    "\n",
    "> Tip: Run **Setup** first to install required packages and download NLTK corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c973a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: /Users/yukkihsu/opt/anaconda3/bin/python\n",
      "transformers: 4.44.2\n",
      "sentence-transformers: 3.0.1\n",
      "umap: 0.5.9.post2\n"
     ]
    }
   ],
   "source": [
    "import sys, transformers, sentence_transformers, umap\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"sentence-transformers:\", sentence_transformers.__version__)\n",
    "print(\"umap:\", umap.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4956c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. _HAS_EMB = True\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk beautifulsoup4 ftfy emoji matplotlib wordcloud scikit-learn umap-learn sentence-transformers datasets transformers\n",
    "\n",
    "import os, json, glob, re\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import ftfy\n",
    "import emoji\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import umap\n",
    "    _HAS_EMB = True\n",
    "except Exception:\n",
    "    _HAS_EMB = False\n",
    "\n",
    "print('Setup complete. _HAS_EMB =', _HAS_EMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6313b900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TOPIC_FRAMES keys: ['Abortion and Reproductive Policy', 'Gun Policy and Firearms Regulation', 'Climate and Environmental Policy'] ...\n"
     ]
    }
   ],
   "source": [
    "# === Configuration ===\n",
    "BASE_DIR = Path('.')  # change if needed\n",
    "DATASET_DIR = BASE_DIR / 'dataset'\n",
    "MERGED_GLOBAL = DATASET_DIR / 'merged_all_posts.json'\n",
    "MERGED_BY_TOPIC_DIR = DATASET_DIR / 'merged_by_topic'\n",
    "FIG_DIR = BASE_DIR / 'figs'\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "URL_RE = re.compile(r'https?://\\S+')\n",
    "MENTION_RE = re.compile(r'@\\w[\\w\\.]*')\n",
    "HASHTAG_RE = re.compile(r'#\\w+')\n",
    "\n",
    "def canonical_id(p):\n",
    "    return p.get('id') or p.get('uri') or p.get('url')\n",
    "\n",
    "def clean_text(html_or_text: str) -> str:\n",
    "    text = BeautifulSoup(html_or_text or '', 'html.parser').get_text(' ')\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = URL_RE.sub(' <URL> ', text)\n",
    "    text = MENTION_RE.sub(' <USER> ', text)\n",
    "    text = HASHTAG_RE.sub(' <HASHTAG> ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text: str):\n",
    "    toks = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "    toks = [w for w in toks if w not in STOPWORDS]\n",
    "    return toks\n",
    "\n",
    "def load_topic_frames():\n",
    "    # optional: brings in your {topic: {'left':[], 'right':[], 'neutral':[]}}\n",
    "    tf_py = BASE_DIR / 'topic_frames.py'\n",
    "    tf_json = BASE_DIR / 'topic_frames.json'\n",
    "    frames = {}\n",
    "    if tf_py.exists():\n",
    "        import importlib.util\n",
    "        spec = importlib.util.spec_from_file_location('topic_frames', str(tf_py))\n",
    "        mod = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(mod)\n",
    "        frames = getattr(mod, 'TOPIC_FRAMES', {})\n",
    "    elif tf_json.exists():\n",
    "        frames = json.loads(tf_json.read_text('utf-8'))\n",
    "    return frames\n",
    "\n",
    "TOPIC_FRAMES = load_topic_frames()\n",
    "print('Loaded TOPIC_FRAMES keys:', list(TOPIC_FRAMES.keys())[:3], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd70da15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded global rows: 38195\n"
     ]
    }
   ],
   "source": [
    "# === Load merged global dataset (if present) ===\n",
    "rows = []\n",
    "if MERGED_GLOBAL.exists():\n",
    "    data = json.loads(MERGED_GLOBAL.read_text('utf-8'))\n",
    "    seen = set()\n",
    "    for p in data:\n",
    "        pid = canonical_id(p)\n",
    "        if not pid or pid in seen:\n",
    "            continue\n",
    "        seen.add(pid)\n",
    "        meta = p.get('__meta__', {}) or {}\n",
    "        txt = clean_text(p.get('content') or p.get('text') or '')\n",
    "        if not txt:\n",
    "            continue\n",
    "        rows.append({\n",
    "            'id': pid,\n",
    "            'text': txt,\n",
    "            'topic': meta.get('topic'),\n",
    "            'keyword': meta.get('matched_keyword'),\n",
    "            'platform': meta.get('platform', 'unknown')\n",
    "        })\n",
    "print(f'Loaded global rows: {len(rows)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f39497be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts: 38195\n",
      "Avg length: 130.22 Median: 68\n",
      "Platforms: Counter({'truthsocial': 38195})\n",
      "Top topics: [('Abortion and Reproductive Policy', 2000), ('Climate and Environmental Policy', 1994), ('Basic Income and Welfare Programs', 1985), ('Foreign Policy and National Defense', 1985), ('Drug Policy and Substance Regulation', 1980), ('Free Speech and Content Regulation', 1979), ('Gun Policy and Firearms Regulation', 1973), ('Immigration and Border Policy', 1953), ('Policing and Criminal Justice Reform', 1951), ('Voting and Election Policy', 1946)]\n",
      "Most common words: [('trump', 16180), ('people', 12970), ('https', 9883), ('government', 9711), ('like', 9254), ('president', 8916), ('america', 8227), ('one', 7703), ('law', 7400), ('us', 7364), ('federal', 7067), ('would', 7050), ('state', 6929), ('american', 6629), ('states', 6361), ('new', 6306), ('url', 6220), ('democrats', 6174), ('public', 5971), ('rights', 5792)]\n",
      "Saved global figures to figs\n"
     ]
    }
   ],
   "source": [
    "# === Global EDA ===\n",
    "import numpy as np\n",
    "\n",
    "lengths = [len(r['text'].split()) for r in rows]\n",
    "print('Total posts:', len(rows))\n",
    "if lengths:\n",
    "    print('Avg length:', round(np.mean(lengths), 2), 'Median:', int(np.median(lengths)))\n",
    "\n",
    "# Plot length distribution\n",
    "plt.figure()\n",
    "plt.hist(lengths, bins=50)\n",
    "plt.title('Text Length Distribution (word count) — Global')\n",
    "plt.xlabel('Words per post')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig(FIG_DIR / 'global_length_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Platform and topic counts\n",
    "from collections import Counter\n",
    "plat_counts = Counter(r['platform'] for r in rows if r.get('platform'))\n",
    "top_counts = Counter(r['topic'] for r in rows if r.get('topic'))\n",
    "print('Platforms:', plat_counts)\n",
    "print('Top topics:', top_counts.most_common(10))\n",
    "\n",
    "# Bar plots\n",
    "plt.figure()\n",
    "plt.bar(list(plat_counts.keys()), list(plat_counts.values()))\n",
    "plt.title('Posts per Platform — Global')\n",
    "plt.savefig(FIG_DIR / 'global_posts_per_platform.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "if top_counts:\n",
    "    labels, vals = zip(*top_counts.most_common(20))\n",
    "    plt.figure(figsize=(8,5))\n",
    "    y_pos = range(len(labels))\n",
    "    plt.barh(y_pos, vals)\n",
    "    plt.yticks(y_pos, labels)\n",
    "    plt.title('Top 20 Topics — Global')\n",
    "    plt.savefig(FIG_DIR / 'global_top_topics.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Tokenization + top words\n",
    "all_tokens = []\n",
    "for r in rows:\n",
    "    all_tokens.extend(tokenize_text(r['text']))\n",
    "freq = Counter(all_tokens)\n",
    "print('Most common words:', freq.most_common(20))\n",
    "\n",
    "# Top words bar chart\n",
    "if freq:\n",
    "    words, counts = zip(*freq.most_common(25))\n",
    "    plt.figure(figsize=(8,5))\n",
    "    y = range(len(words))\n",
    "    plt.barh(y, counts)\n",
    "    plt.yticks(y, words)\n",
    "    plt.title('Top 25 Words — Global')\n",
    "    plt.savefig(FIG_DIR / 'global_top_words.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print('Saved global figures to', FIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6461b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sentiment (VADER) ===\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sent = [sia.polarity_scores(r['text'])['compound'] for r in rows]\n",
    "plt.figure()\n",
    "plt.hist(sent, bins=50)\n",
    "plt.title('Sentiment (compound) — Global')\n",
    "plt.savefig(FIG_DIR / 'global_sentiment_hist.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# By platform boxplot-like summary (manual)\n",
    "by_platform = defaultdict(list)\n",
    "for r, s in zip(rows, sent):\n",
    "    by_platform[r['platform']].append(s)\n",
    "\n",
    "labels = list(by_platform.keys())\n",
    "plt.figure()\n",
    "data = [by_platform[k] for k in labels]\n",
    "plt.boxplot(data, labels=labels, showmeans=True)\n",
    "plt.title('Sentiment by Platform — Global')\n",
    "plt.savefig(FIG_DIR / 'global_sentiment_by_platform.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ca3316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yukkihsu/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 250/250 [03:27<00:00,  1.21it/s]\n",
      "/Users/yukkihsu/opt/anaconda3/lib/python3.9/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/Users/yukkihsu/opt/anaconda3/lib/python3.9/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12040. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n",
      "OMP: Info #271: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "# === UMAP Embeddings (optional: requires sentence-transformers & umap) ===\n",
    "if _HAS_EMB and rows:\n",
    "    texts = [r['text'] for r in rows]\n",
    "    platforms = [r['platform'] for r in rows]\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    import numpy as np\n",
    "    n = len(texts)\n",
    "    if n > 8000:\n",
    "        idx = np.random.choice(n, 8000, replace=False)\n",
    "        texts = [texts[i] for i in idx]\n",
    "        platforms = [platforms[i] for i in idx]\n",
    "    emb = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)\n",
    "    xy = reducer.fit_transform(emb)\n",
    "    # Scatter by platform\n",
    "    plt.figure(figsize=(6,5))\n",
    "    labs = sorted(set(platforms))\n",
    "    for lab in labs:\n",
    "        idx = [i for i, p in enumerate(platforms) if p == lab]\n",
    "        plt.scatter(xy[idx,0], xy[idx,1], s=4, label=lab, alpha=0.7)\n",
    "    plt.title('UMAP — colored by platform (global)')\n",
    "    plt.legend(markerscale=3)\n",
    "    plt.savefig(FIG_DIR / 'global_umap_platform.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "else:\n",
    "    print('UMAP skipped: sentence-transformers/umap not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d0d93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TF-IDF top terms by platform ===\n",
    "platform_buckets = defaultdict(list)\n",
    "for r in rows:\n",
    "    platform_buckets[r['platform']].append(r['text'])\n",
    "\n",
    "for name, texts in platform_buckets.items():\n",
    "    if len(texts) < 50:\n",
    "        continue\n",
    "    vec = TfidfVectorizer(lowercase=True, ngram_range=(1,2), min_df=5, max_df=0.5,\n",
    "                          token_pattern=r\"[A-Za-z<>#@][A-Za-z0-9_<>\\-']+\")\n",
    "    X = vec.fit_transform(texts)\n",
    "    scores = X.mean(axis=0).A1\n",
    "    terms = vec.get_feature_names_out()\n",
    "    top = sorted(zip(terms, scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "    # Plot\n",
    "    terms_plot = [t for t, _ in top][::-1]\n",
    "    vals_plot = [s for _, s in top][::-1]\n",
    "    plt.figure(figsize=(7,5))\n",
    "    y = range(len(terms_plot))\n",
    "    plt.barh(y, vals_plot)\n",
    "    plt.yticks(y, terms_plot)\n",
    "    plt.title(f'Top TF-IDF terms — {name}')\n",
    "    plt.savefig(FIG_DIR / f'tfidf_{name}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "608449f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found topic files: 20\n",
      "Topic-level figures saved to figs\n"
     ]
    }
   ],
   "source": [
    "# === Topic-level EDA (iterate merged_by_topic/*.json if present) ===\n",
    "topic_files = sorted(glob.glob(str(MERGED_BY_TOPIC_DIR / '*_posts.json')))\n",
    "print('Found topic files:', len(topic_files))\n",
    "\n",
    "for path in topic_files:\n",
    "    topic_slug = Path(path).name.replace('_posts.json','')\n",
    "    data = json.loads(Path(path).read_text('utf-8'))\n",
    "    topic_rows = []\n",
    "    seen_ids = set()\n",
    "    for p in data:\n",
    "        pid = canonical_id(p)\n",
    "        if not pid or pid in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(pid)\n",
    "        meta = p.get('__meta__', {}) or {}\n",
    "        txt = clean_text(p.get('content') or p.get('text') or '')\n",
    "        if not txt:\n",
    "            continue\n",
    "        topic_rows.append({'id': pid, 'text': txt, 'platform': meta.get('platform','unknown')})\n",
    "    if not topic_rows:\n",
    "        continue\n",
    "\n",
    "    # length dist\n",
    "    lengths = [len(r['text'].split()) for r in topic_rows]\n",
    "    plt.figure()\n",
    "    plt.hist(lengths, bins=50)\n",
    "    plt.title(f'Text Length — {topic_slug}')\n",
    "    plt.savefig(FIG_DIR / f'{topic_slug}_length.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # tokens top words\n",
    "    tokens = []\n",
    "    for r in topic_rows:\n",
    "        tokens.extend(tokenize_text(r['text']))\n",
    "    freq = Counter(tokens)\n",
    "    if freq:\n",
    "        words, counts = zip(*freq.most_common(20))\n",
    "        plt.figure(figsize=(7,5))\n",
    "        y = range(len(words))\n",
    "        plt.barh(y, counts)\n",
    "        plt.yticks(y, words)\n",
    "        plt.title(f'Top 20 words — {topic_slug}')\n",
    "        plt.savefig(FIG_DIR / f'{topic_slug}_top_words.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "print('Topic-level figures saved to', FIG_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
